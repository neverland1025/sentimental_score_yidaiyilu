{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snownlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "\n",
    "#######read LM and NTUSD dict#########\n",
    "\n",
    "LM_P=pd.read_excel('LM词典+NTUSD词典.xlsx',header=None,index_col=0,sheet_name='LM_Positive（CNRDS整理）')\n",
    "LM_P=LM_P.reset_index().to_dict()\n",
    "LM_P={value:key for key, value in LM_P[0].items()}\n",
    "LM_P_key= {v : k for k, v in LM_P.items()}\n",
    "LM_N=pd.read_excel('LM词典+NTUSD词典.xlsx',index_col=0,header=None,sheet_name='LM_Negative（CNRDS整理）')\n",
    "LM_N=LM_N.reset_index().to_dict()\n",
    "LM_N={value:key for key, value in LM_N[0].items()}\n",
    "LM_N_key= {v : k for k, v in LM_N.items()}\n",
    "NTUSD_P=pd.read_excel('LM词典+NTUSD词典.xlsx',index_col=0,header=None,sheet_name='NTUSD_Positive')\n",
    "NTUSD_P=NTUSD_P.reset_index().to_dict()\n",
    "NTUSD_P={value:key for key, value in NTUSD_P[0].items()}\n",
    "NTUSD_P_key= {v : k for k, v in NTUSD_P.items()}\n",
    "NTUSD_N=pd.read_excel('LM词典+NTUSD词典.xlsx',index_col=0,header=None,sheet_name='NTUSD_Negative')\n",
    "NTUSD_N=NTUSD_N.reset_index().to_dict()\n",
    "NTUSD_N={value:key for key, value in NTUSD_N[0].items()}\n",
    "NTUSD_N_key= {v : k for k, v in NTUSD_N.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## read different excel ##########\n",
    "\n",
    "####### enter excel name here ##########\n",
    "#excel='MDA_br'\n",
    "excel='report_br'\n",
    "\n",
    "####### enter the volume of the dataset here (percentage) #########\n",
    "percentage=95\n",
    "#percentage=99\n",
    "\n",
    "####### use the whole sentence or several word around '一带一路' ##########\n",
    "model='whole'\n",
    "#model='substring'\n",
    "\n",
    "######enter forward and backward words around '一带一路'##############\n",
    "######if model is whole please still keep number there, it won't affect the result ########\n",
    "number=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######To find all substring in one string#########\n",
    "\n",
    "def find_all(str1,key):\n",
    "    lstKey = [] #定义空列表用于存储多个指定字符的索引\n",
    "    lengthKey = 0\n",
    "    #字符串中存在指定字符串的个数\n",
    "    countStr = str1.count(key)\n",
    "\n",
    "    #利用获取的countStr进行判断\n",
    "    if countStr < 1:\n",
    "        return -1\n",
    "    elif countStr == 1: #当字符串中只有一个指定字符时，直接通过find()方法即可解决\n",
    "        indexKey = str1.find(key)\n",
    "        return indexKey\n",
    "    else: #当字符串中存在多个指定字符的处理方法\n",
    "        #第一个指定字符的处理方法\n",
    "        indexKey = str1.find(key)\n",
    "        lstKey.append(indexKey) #将第一个索引加入到lstKey列表中\n",
    "        #其余指定字符的处理方法\n",
    "        while countStr > 1:\n",
    "            #将前一个指定字符之后的字符串截取下来\n",
    "            str_new = str1[indexKey+1:len(str1)+1]\n",
    "            #获取截取后的字符串中前一个指定字符的索引值\n",
    "            indexKey_new = str_new.find(key)\n",
    "            #后一个索引值=前一个索引值+1+indexkey_new\n",
    "            indexKey = indexKey+1 +indexKey_new\n",
    "            #将后发现的索引值加入lstKey列表中\n",
    "            lstKey.append(indexKey)\n",
    "            countStr -= 1\n",
    "        #print('查找的关键字的索引为',lstKey)\n",
    "        return lstKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def find_yidaiyilu(string1):\n",
    "    if type(string1)==float:\n",
    "        return 'None'\n",
    "    if string1[-1]!='。' or '，':\n",
    "        string1=string1+'。'\n",
    "        \n",
    "    \n",
    "    pattern=re.compile('[，。、,.;:][^，。]*一带一路[^，。、,.:;]*[，。,.:;]')\n",
    "    search = pattern.search(string1)\n",
    "    if search!=None:\n",
    "        print(search.group(0))\n",
    "        return search.group(0)\n",
    "    else:\n",
    "        return 'None'\n",
    "    \n",
    "######### if want to use snownlp for sentimental score #########   \n",
    "def sent_find(string1):\n",
    "    if type(string1)==float:\n",
    "        return 'None'\n",
    "    \n",
    "    t = SnowNLP(string1)\n",
    "    list1=[]\n",
    "    for sen in t.sentences:\n",
    "        list1.append(sen+'。')\n",
    "   \n",
    "    for i in list1:\n",
    "        if type(i)==float:\n",
    "            continue\n",
    "        if i.find('一带一路')!=-1:\n",
    "            return (i,len(i))\n",
    "        \n",
    "        \n",
    "    return 'None'\n",
    "\n",
    "########### find a few words before or after the substring ###########\n",
    "def cut_ydyl(string1,num):\n",
    "    if type(string1)==float:\n",
    "        return ['None']\n",
    "    lst=find_all(string1,'一带一路')\n",
    "    if type(lst)==int:\n",
    "        if lst==-1:\n",
    "            return ['None']\n",
    "        else:\n",
    "            if lst-num<0:\n",
    "                return [string1[:lst+num]]\n",
    "            else:\n",
    "                return [string1[lst-num:lst+num]]\n",
    "    list1=[]\n",
    "    n=0\n",
    "    if type(lst)==list:\n",
    "        for n in range(0,len(lst)):\n",
    "            if n==len(lst)-1:\n",
    "                continue\n",
    "            elif lst[n]+num>=lst[n+1]:\n",
    "                    lst[n+1]=-1\n",
    "        lst=[n for n in lst if n>=0]\n",
    "        \n",
    "        for n in range(0,len(lst)):\n",
    "                \n",
    "            if lst[n]-num<0:\n",
    "                list1.append(string1[:lst[n]+num+len('一带一路')])\n",
    "            else:\n",
    "                list1.append(string1[lst[n]-num:lst[n]+num+len('一带一路')])\n",
    "                \n",
    "    return list1\n",
    " \n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "def get_key (dict, value):\n",
    "    return [k for k, v in dict.items() if v == value]         \n",
    "def search(dict, value):\n",
    "    return dict.get(value)\n",
    "def search_pos_neg(value):\n",
    "    \n",
    "    if search(LM_P,value)!=None or search(NTUSD_P,value)!=None:\n",
    "        return 1\n",
    "    if search(NTUSD_N,value)!=None or search(LM_N,value)!=None:\n",
    "        return 0\n",
    "\n",
    "                \n",
    "        \n",
    "####### calculate sentimental score LM_TONE NTUSD_TONE##########    \n",
    "def sentimental(i,tone):\n",
    "    list1=i.split('/')\n",
    "    pscore=0\n",
    "    nscore=0\n",
    "    total=len(list1)\n",
    "    list2=[]\n",
    "    list3=[]\n",
    "    list_p=[]\n",
    "    list_n=[]\n",
    "    if tone=='LM_TONE1':\n",
    "\n",
    "        for a in list1:\n",
    "            if search(LM_P,a)!=None:\n",
    "                pscore+=1\n",
    "                list2.append(a)\n",
    "                list_p.append((a,1))\n",
    "            if search(LM_N,a)!=None:\n",
    "                nscore += 1\n",
    "                list2.append(a)\n",
    "                list_n.append((a,1))\n",
    "        if pscore + nscore == 0:\n",
    "            LM_TONE1=0\n",
    "            return (pscore,nscore,LM_TONE1,list2,list_p,list_n)\n",
    "        else:\n",
    "            LM_TONE1=(pscore-nscore)/total\n",
    "            return (pscore,nscore,LM_TONE1,list2,list_p,list_n)\n",
    "    if tone==\"LM_TONE2\":\n",
    "        for a in list1:\n",
    "            if search(LM_P,a)!=None:\n",
    "                pscore+=1\n",
    "            if search(LM_N,a)!=None:\n",
    "                nscore += 1\n",
    "        if pscore + nscore == 0:\n",
    "            LM_TONE2=0\n",
    "            return (pscore,nscore,LM_TONE2)\n",
    "        else:\n",
    "            LM_TONE2=(pscore-nscore)/(pscore+nscore)\n",
    "            return (pscore,nscore,LM_TONE2)\n",
    "    if tone=='NTUSD_TONE':\n",
    "        for a in list1:\n",
    "            if search(NTUSD_P,a)!=None:\n",
    "                pscore+=1\n",
    "                list3.append(a)\n",
    "                list_p.append((a,1))\n",
    "            if search(NTUSD_N,a)!=None:\n",
    "                nscore += 1\n",
    "                list3.append(a)\n",
    "                list_n.append((a,1))\n",
    "        if pscore+nscore==0:\n",
    "            NTUSD_TONE=0\n",
    "            return (pscore,nscore,NTUSD_TONE,list3,list_p,list_n)\n",
    "        else:\n",
    "            NTUSD_TONE=(pscore-nscore)/(pscore+nscore)\n",
    "            return (pscore,nscore,NTUSD_TONE,list3,list_p,list_n)\n",
    "def seperate(tuple1,n):\n",
    "    print(type(tuple1))\n",
    "    x=tuple1[n]\n",
    "    return x\n",
    "\n",
    "def token(string):\n",
    "    text = jieba.cut(string.strip())\n",
    "    return '/'.join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######if use find 10 words forward and 10 words backward########\n",
    "def substring_model(df,num):\n",
    "    list1=[]\n",
    "\n",
    "    df['string']=df['内容'].apply(lambda x:cut_ydyl(x,num))\n",
    "    df=df.drop(columns=['内容'])\n",
    "\n",
    "\n",
    "    #df=df.reindex(df.index.repeat(df['string'].str.len()))\n",
    "    #df=df.assign(B=np.concatenate(df.string.values))\n",
    "    df=df.reindex(df.index.repeat(df['string'].str.len())).assign(string=np.concatenate(df.string.values))\n",
    "\n",
    "\n",
    "    ######if use sent_find#########\n",
    "    for x in df['string']:\n",
    "        if x is not str:\n",
    "            x=str(x)\n",
    "            text = token(x.strip())\n",
    "            list1.append(text)\n",
    "    df_cut=pd.DataFrame(list1,columns=['cut'])\n",
    "    df=pd.concat([df.reset_index(),df_cut],axis=1)\n",
    "    df['len']=df['cut'].apply(lambda x:len(x))\n",
    "    df=df.drop(columns=['index'])\n",
    "    return df\n",
    "#######if use whole sentence############\n",
    "def whole_model(df):\n",
    "    list1=[]\n",
    "    df['string']=df['内容']\n",
    "   \n",
    "\n",
    "    for x in df['string']:\n",
    "        if x is not str:\n",
    "            x=str(x)\n",
    "            text = token(x.strip())\n",
    "            list1.append(text)\n",
    "    df_cut=pd.DataFrame(list1,columns=['cut'])\n",
    "    df=pd.concat([df.reset_index(),df_cut],axis=1)\n",
    "    df['len']=df['cut'].apply(lambda x:len(x))\n",
    "    df=df.drop(columns=['index'])\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## sort to find the top n hot word ##########\n",
    "\n",
    "import collections\n",
    "\n",
    "from string import digits\n",
    "\n",
    "def counter(text,n):\n",
    "    \n",
    "   \n",
    "    \n",
    "    while '' in text:\n",
    "        text.remove('')\n",
    "    freq=collections.Counter(text)\n",
    "    frequency={k: v for k, v in freq.items() if not k.isdigit()}\n",
    "    if len(frequency)>=n:\n",
    "        \n",
    "        return sorted(frequency.items(),key=lambda kv:(kv[1],kv[0]),reverse=True)[:100]\n",
    "    else:\n",
    "        return sorted(frequency.items(),key=lambda kv:(kv[1],kv[0]),reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_zero(list1,num):\n",
    "    while len(list1)<num:\n",
    "        list1.append(0)\n",
    "    return list1\n",
    "\n",
    "########calculate by firm year sentimental score ###########\n",
    "def by_firm_year(df_fy):\n",
    "    \n",
    "\n",
    "    df_fy['NTUSD_TONE']=df_fy['cut'].apply(lambda x:sentimental(x,'NTUSD_TONE'))\n",
    "    df_fy['Number of Positive terms NTUSD']=df_fy['NTUSD_TONE'].apply(lambda x:x[0])\n",
    "    df_fy['Number of Negative terms NTUSD']=df_fy['NTUSD_TONE'].apply(lambda x:x[1])\n",
    "    df_fy['NTUSD_total']=df_fy['NTUSD_TONE'].apply(lambda x:x[3])\n",
    "    df_fy['NTUSD_positive']=df_fy['NTUSD_TONE'].apply(lambda x:x[4])\n",
    "    df_fy['NTUSD_negative']=df_fy['NTUSD_TONE'].apply(lambda x:x[5])\n",
    "    df_fy['NTUSD_TONE']=df_fy['NTUSD_TONE'].apply(lambda x:x[2])\n",
    "\n",
    "\n",
    "    df_fy['LM_TONE1']=df_fy['cut'].apply(lambda x:sentimental(x,'LM_TONE1'))\n",
    "    df_fy['LM_TONE2']=df_fy['cut'].apply(lambda x:sentimental(x,'LM_TONE2'))\n",
    "    df_fy['Number of Positive terms LM_TONE']=df_fy['LM_TONE1'].apply(lambda x:x[0])\n",
    "    df_fy['Number of Negative terms LM_TONE']=df_fy['LM_TONE1'].apply(lambda x:x[1])\n",
    "    df_fy['LM_total']=df_fy['LM_TONE1'].apply(lambda x:x[3])\n",
    "    df_fy['LM_positive']=df_fy['LM_TONE1'].apply(lambda x:x[4])\n",
    "    df_fy['LM_negative']=df_fy['LM_TONE1'].apply(lambda x:x[5])\n",
    "    df_fy['LM_TONE1']=df_fy['LM_TONE1'].apply(lambda x:x[2])\n",
    "    df_fy['LM_TONE2']=df_fy['LM_TONE2'].apply(lambda x:x[2])\n",
    "    \n",
    "    df_fy=df_fy.drop(columns=['cut'])\n",
    "    return df_fy\n",
    "\n",
    "def dumlist(i):\n",
    "    str=''\n",
    "    for x in i:\n",
    "        str+=x[0]\n",
    "        #str+='/''\n",
    "    return str\n",
    "\n",
    "####### out put negative words and the orginal sentence #######\n",
    "def original_sentence_negative_words(df,excel,model):\n",
    "    df_n=by_firm_year(df)\n",
    "    df_n['NTUSD_empty']=df_n[\"NTUSD_negative\"].apply(lambda x: len(x)!=0)\n",
    "    df_n['LM_empty']=df_n['LM_negative'].apply(lambda x:len(x)!=0)\n",
    "    df_ntusd=df_n[df_n['NTUSD_empty']]\n",
    "\n",
    "    df_l=pd.DataFrame(df_ntusd['NTUSD_negative'].tolist(), index=df_ntusd.index)\n",
    "    df_l=df_l[0].apply(lambda x :x[0])\n",
    "    df_ntusd=pd.concat([df_ntusd,df_l],axis=1)\n",
    "    df_ntusd=df_ntusd.groupby([0]).agg(lambda x:'#'.join(x))['string']\n",
    "   \n",
    "    df_lm=df_n[df_n['LM_empty']]\n",
    "    df_lm=pd.DataFrame(df_lm['LM_negative'].tolist(), index=df_lm.index)\n",
    "    df_lm=df_lm[0].apply(lambda x :x[0])\n",
    "    df_n=pd.concat([df_n,df_lm],axis=1)\n",
    "    df_n=df_n.groupby([0]).agg(lambda x:'#'.join(x))['string']\n",
    "    \n",
    "    if model=='substring':\n",
    "        df_ntusd=df_ntusd.to_frame().reset_index().to_excel(excel+'_negative_orgin_ntusd.xls')\n",
    "        df_lm=df_n.to_frame().reset_index().to_excel(excel+'_negative_origin_lm.xls')\n",
    "    if model=='whole':\n",
    "        df_ntusd=df_ntusd.to_frame().reset_index().to_csv(excel+'_negative_orgin_ntusd.csv')\n",
    "        df_lm=df_n.to_frame().reset_index().to_csv(excel+'_negative_origin_lm.csv')\n",
    "        \n",
    "    \n",
    "######## out put sentimental score 'report' && 'describe' && 'top n hot word' for excel ########\n",
    "def quantile_fy(num,df,excel):\n",
    "    df_fy_95=df[df['len']<=df['len'].quantile(num/100)]\n",
    "    df_fy_95=by_firm_year(df_fy_95)\n",
    "    df_fy_95.to_excel(excel+str(num)+'_report'+'.xls')\n",
    "    df_fy_95.describe().to_excel(excel+str(num)+'_report'+'_describe.xls')\n",
    "    df_all=df_fy_95['NTUSD_total'].sum()\n",
    "    cut=counter(df_all,100)\n",
    "    NTUSD_total=df_fy_95['Number of Positive terms NTUSD'].sum()+df_fy_95['Number of Negative terms NTUSD'].sum()\n",
    "    LM_total=df_fy_95['Number of Positive terms LM_TONE'].sum()+df_fy_95['Number of Negative terms LM_TONE'].sum()\n",
    "    \n",
    "    \n",
    "    #cut_df=pd.DataFrame(columns=['NTUSD_Negative_word','NTUSD_times','LM_Negative_word','LM_times'])\n",
    "    cut_df=pd.DataFrame()\n",
    "    cut_df['NTUSD_word']=pd.Series(append_zero([x[0] for x in cut],100))\n",
    "    cut_df['NTUSD_negative/postive']=cut_df['NTUSD_word'].apply(lambda x: search_pos_neg(x))\n",
    "    cut_df['NTUSD_times']=pd.Series(append_zero([x[1] for x in cut],100))\n",
    "    cut_df['NTUSD_percentage']=cut_df['NTUSD_times']/NTUSD_total\n",
    "    df_all=df_fy_95['LM_total'].sum()\n",
    "    cut=counter(df_all,100)\n",
    "    cut_df['LM_word']=pd.Series(append_zero([x[0] for x in cut],100))\n",
    "    cut_df['LM_negative/postive']=cut_df['LM_word'].apply(lambda x: search_pos_neg(x))\n",
    "    cut_df['LM_times']=pd.Series(append_zero([x[1] for x in cut],100))\n",
    "    cut_df['LM_percentage']=cut_df['LM_times']/LM_total\n",
    "    \n",
    "    \n",
    "    cut_df.to_excel(excel+str(num)+'_report'+'_top100.xls')\n",
    "    return(df_fy_95,df_fy_95.describe(),cut_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/qc/8vvdn0tj149b1yz8g8qm304h0000gn/T/jieba.cache\n",
      "Loading model cost 0.697 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "def sentimental_score(excel,percentage,model,number):\n",
    "    df=pd.read_excel(excel+'.xlsx',index_col=0,decode='unicode').dropna().reset_index()\n",
    "    if model=='substring':\n",
    "        df=substring_model(df,number)\n",
    "        original_sentence_negative_words(df,excel,model)\n",
    "        quantile_fy(percentage,df,excel)\n",
    "    if model=='whole':\n",
    "        df=whole_model(df)\n",
    "        original_sentence_negative_words(df,excel,model)\n",
    "        quantile_fy(percentage,df,excel)\n",
    "\n",
    "sentimental_score(excel,percentage,model,number)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
